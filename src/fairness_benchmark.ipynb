{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17389aa4-0c8a-4234-ac52-d007c4ec5a31",
   "metadata": {},
   "source": [
    "## Setting Sex, Rce, Sex+Race, Age+Education as protected attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83928909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "\n",
    "# Suppress PyTorch warnings about weights_only\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.serialization\")\n",
    "\n",
    "# Try to import tabulate, fallback to simple printing if not available\n",
    "try:\n",
    "    from tabulate import tabulate\n",
    "    HAS_TABULATE = True\n",
    "except ImportError:\n",
    "    HAS_TABULATE = False\n",
    "    print(\"tabulate library not found. Using simple table formatting.\")\n",
    "\n",
    "class FCNN(nn.Module):\n",
    "    \"\"\"Standard FCNN model for non-quantized models\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the appropriate device (CUDA if available, else CPU)\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Load a PyTorch model with special handling for different model formats\n",
    "    \"\"\"\n",
    "    try:\n",
    "        device = get_device()\n",
    "        \n",
    "        # Check if this is a quantized model based on filename\n",
    "        is_quantized = any(q in model_path for q in ['INT4', 'INT8', 'INT16'])\n",
    "        \n",
    "        if is_quantized:\n",
    "            # print(f\"Loading quantized model: {model_path}\")\n",
    "            try:\n",
    "                # Load as state_dict first\n",
    "                state_dict = torch.load(model_path, map_location='cpu')\n",
    "                \n",
    "                # Check if it's a state_dict or a model\n",
    "                if isinstance(state_dict, dict):\n",
    "                    # print(\"Loaded state dictionary for quantized model\")\n",
    "                    \n",
    "                    # We'll recreate the original model structure and load these weights later\n",
    "                    return state_dict, device, True, True  # Last True indicates it's a dict\n",
    "                else:\n",
    "                    # print(\"Loaded quantized model object\")\n",
    "                    if hasattr(state_dict, 'eval'):\n",
    "                        state_dict.eval()\n",
    "                    return state_dict, device, True, False  # False indicates it's not a dict\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading quantized model: {str(e)}\")\n",
    "                raise\n",
    "        else:\n",
    "            # Handle regular model loading\n",
    "            # print(f\"Loading regular model: {model_path}\")\n",
    "            state_dict = torch.load(model_path, map_location='cpu')\n",
    "            \n",
    "            # Get input dimension from fc1 weight matrix\n",
    "            input_dim = state_dict['fc1.weight'].shape[1]\n",
    "            hidden_dim = state_dict['fc1.weight'].shape[0]\n",
    "            \n",
    "            # Create model with correct dimensions\n",
    "            model = FCNN(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "            \n",
    "            # Load state dict\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.eval()\n",
    "            \n",
    "            # Move to appropriate device\n",
    "            if device.type == 'cuda':\n",
    "                try:\n",
    "                    model = model.to(device)\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Warning: Could not move model to CUDA. Using CPU instead. Error: {e}\")\n",
    "                    device = torch.device('cpu')\n",
    "            \n",
    "            return model, device, False, False  # Not quantized, not a dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in load_model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def prepare_adult_dataset(expected_features=14, protected_attribute='sex'):\n",
    "    \"\"\"\n",
    "    Prepare Adult dataset with proper formatting for fairness analysis\n",
    "    \"\"\"\n",
    "    # Define column names\n",
    "    column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                   'marital-status', 'occupation', 'relationship', 'race', \n",
    "                   'sex', 'capital-gain', 'capital-loss', 'hours-per-week', \n",
    "                   'native-country', 'income']\n",
    "    \n",
    "    # Load data\n",
    "    data_path = '../data/raw/adult.csv'\n",
    "    if not os.path.exists(data_path):\n",
    "        data_path = '../../data/raw/adult.csv'  # Try alternate path\n",
    "    \n",
    "    df = pd.read_csv(data_path, \n",
    "                     header=None,\n",
    "                     names=column_names,\n",
    "                     skipinitialspace=True)\n",
    "    \n",
    "    # Clean the data\n",
    "    string_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in string_columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "    \n",
    "    # Ensure income is properly encoded\n",
    "    df['income'] = (df['income'].str.contains('>50K')).astype(int)\n",
    "    \n",
    "    # Create base numerical features\n",
    "    numerical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain',\n",
    "                         'capital-loss', 'hours-per-week']\n",
    "                         \n",
    "    # Create all protected attribute variations\n",
    "    # 1. Sex (binary)\n",
    "    df['sex_binary'] = (df['sex'] == 'Male').astype(int)\n",
    "    \n",
    "    # 2. Race (binary: White vs non-White)\n",
    "    df['race_binary'] = (df['race'] == 'White').astype(int)\n",
    "    \n",
    "    # 3. Race + Sex (intersectional)\n",
    "    df['race_sex'] = ((df['race'] == 'White') & (df['sex'] == 'Male')).astype(int)\n",
    "    \n",
    "    # 4. Age + Education (intersectional)\n",
    "    df['higher_edu'] = (df['education-num'] >= 12).astype(int)\n",
    "    df['older'] = (df['age'] >= 40).astype(int)\n",
    "    df['age_edu'] = ((df['age'] >= 40) & (df['education-num'] >= 12)).astype(int)\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "    \n",
    "    # Determine which protected attribute to use\n",
    "    if protected_attribute == 'sex':\n",
    "        actual_attribute = 'sex_binary'\n",
    "    elif protected_attribute == 'race':\n",
    "        actual_attribute = 'race_binary'\n",
    "    elif protected_attribute == 'race+sex':\n",
    "        actual_attribute = 'race_sex'\n",
    "    elif protected_attribute == 'age+education':\n",
    "        actual_attribute = 'age_edu'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported protected attribute: {protected_attribute}\")\n",
    "    \n",
    "    # To ensure consistency, include the protected attribute with its original name\n",
    "    df[protected_attribute] = df[actual_attribute]\n",
    "    \n",
    "    # Start with numerical features plus the selected protected attribute\n",
    "    base_features = numerical_features + [protected_attribute]\n",
    "    final_df = df[base_features + ['income']]\n",
    "    \n",
    "    # If we need more features to match expected dimension\n",
    "    if expected_features > len(base_features):\n",
    "        # Add categorical features one at a time until we have enough\n",
    "        categorical_features = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                               'relationship', 'native-country']\n",
    "        \n",
    "        # Don't include race or sex again if they're part of our protected attribute\n",
    "        if protected_attribute == 'race' or protected_attribute == 'race+sex':\n",
    "            if 'race' in categorical_features:\n",
    "                categorical_features.remove('race')\n",
    "                \n",
    "        if protected_attribute == 'sex' or protected_attribute == 'race+sex':\n",
    "            if 'sex' in categorical_features:\n",
    "                categorical_features.remove('sex')\n",
    "        \n",
    "        for cat_feature in categorical_features:\n",
    "            if len(final_df.columns) - 1 >= expected_features:  # -1 for 'income'\n",
    "                break\n",
    "                \n",
    "            # Add this categorical feature\n",
    "            cat_encoded = pd.get_dummies(df[cat_feature], prefix=cat_feature)\n",
    "            final_df = pd.concat([final_df.drop('income', axis=1), \n",
    "                                cat_encoded, \n",
    "                                final_df['income']], axis=1)\n",
    "    \n",
    "    # If we have too many features, select only the needed amount\n",
    "    if len(final_df.columns) - 1 > expected_features:  # -1 for 'income'\n",
    "        # Always ensure the protected attribute is included\n",
    "        keep_cols = [protected_attribute]\n",
    "        \n",
    "        # Add other columns until we reach the expected number\n",
    "        remaining_cols = [c for c in final_df.columns if c != protected_attribute and c != 'income']\n",
    "        keep_cols.extend(remaining_cols[:expected_features - 1])\n",
    "        \n",
    "        # Add income back\n",
    "        keep_cols.append('income')\n",
    "        final_df = final_df[keep_cols]\n",
    "    \n",
    "    # Convert all columns to float32 for PyTorch compatibility\n",
    "    final_df = final_df.astype('float32')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def compute_fairness_metrics(model_or_dict, device, dataset, protected_attribute, privileged_groups=None, \n",
    "                           is_quantized=False, is_dict=False, input_dim=None):\n",
    "    \"\"\"\n",
    "    Compute fairness metrics with special handling for dictionary models\n",
    "    \"\"\"\n",
    "    if privileged_groups is None:\n",
    "        privileged_groups = [{protected_attribute: 1}]\n",
    "    \n",
    "    # Ensure all data is float32\n",
    "    dataset = dataset.astype('float32')\n",
    "    \n",
    "    # Convert to AIF360 format\n",
    "    aif_dataset = BinaryLabelDataset(\n",
    "        df=dataset,\n",
    "        label_names=['income'],\n",
    "        protected_attribute_names=[protected_attribute],\n",
    "        privileged_protected_attributes=[[1]]\n",
    "    )\n",
    "    \n",
    "    # Prepare input features\n",
    "    X = torch.FloatTensor(dataset.drop('income', axis=1).values)\n",
    "    \n",
    "    # Move to appropriate device\n",
    "    if device.type == 'cuda':\n",
    "        try:\n",
    "            X = X.to(device)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Warning: Could not move input to CUDA. Using CPU instead. Error: {e}\")\n",
    "            X = X.cpu()\n",
    "            device = torch.device('cpu')\n",
    "    \n",
    "    # If model is a dict, we need to manually apply the operations\n",
    "    if is_dict:\n",
    "        # Recreate a model structure with the right dimensions\n",
    "        if input_dim is None:\n",
    "            input_dim = X.shape[1]  # Use input dimension from dataset\n",
    "            \n",
    "        # Create a model with the right structure\n",
    "        model = FCNN(input_dim=input_dim, hidden_dim=64)\n",
    "        \n",
    "        # Try to load weights from the state dict\n",
    "        try:\n",
    "            # Check if the dict has expected keys\n",
    "            if 'fc1.weight' in model_or_dict and 'fc2.weight' in model_or_dict:\n",
    "                model.load_state_dict(model_or_dict)\n",
    "            elif 'state_dict' in model_or_dict:\n",
    "                # Some models save state dict under 'state_dict' key\n",
    "                model.load_state_dict(model_or_dict['state_dict'])\n",
    "            else:\n",
    "                print(\"WARNING: Could not load state dict, using random weights\")\n",
    "                \n",
    "            model.eval()\n",
    "            model = model.to(device)\n",
    "            model_or_dict = model  # Use the recreated model\n",
    "            is_dict = False  # Not a dict anymore\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading weights from dict: {str(e)}\")\n",
    "            print(\"Using random predictions as fallback\")\n",
    "            is_dict = True  # Still a dict, will use random predictions\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = 1000\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # For dictionary models that couldn't be converted, use random predictions\n",
    "                if is_dict:\n",
    "                    pred = np.random.random(batch.size(0))\n",
    "                # For regular models or successfully converted dict models\n",
    "                else:\n",
    "                    pred = model_or_dict(batch)\n",
    "                    \n",
    "                    # Ensure we have the right shape and move to CPU\n",
    "                    pred = pred.squeeze().cpu().numpy()\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during batch prediction: {str(e)}\")\n",
    "                # Use random predictions as fallback\n",
    "                pred = np.random.random(batch.size(0))\n",
    "                predictions.append(pred)\n",
    "    \n",
    "    y_pred = (np.concatenate(predictions) > 0.5).astype(float)\n",
    "    y_true = dataset['income'].values\n",
    "    \n",
    "    # Calculate fairlearn metrics\n",
    "    dem_parity = demographic_parity_difference(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        sensitive_features=dataset[protected_attribute]\n",
    "    )\n",
    "    \n",
    "    eq_odds = equalized_odds_difference(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        sensitive_features=dataset[protected_attribute]\n",
    "    )\n",
    "    \n",
    "    # Create classified dataset with model predictions\n",
    "    classified_dataset = aif_dataset.copy()\n",
    "    classified_dataset.labels = y_pred.reshape(-1, 1)\n",
    "    \n",
    "    # Use classification_metric for all fairness calculations\n",
    "    classification_metric = ClassificationMetric(\n",
    "        aif_dataset,\n",
    "        classified_dataset,\n",
    "        unprivileged_groups=[{protected_attribute: 0}],\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "    \n",
    "    # Return metrics based on predictions\n",
    "    return {\n",
    "        'disparate_impact': classification_metric.disparate_impact(),\n",
    "        'statistical_parity_difference': classification_metric.statistical_parity_difference(),\n",
    "        'demographic_parity_difference': dem_parity,\n",
    "        'equalized_odds_difference': eq_odds,\n",
    "        'average_odds_difference': classification_metric.average_odds_difference(),\n",
    "        'equal_opportunity_difference': classification_metric.equal_opportunity_difference(),\n",
    "        'theil_index': classification_metric.theil_index()\n",
    "    }\n",
    "\n",
    "def analyze_model_fairness(model_path, protected_attribute='sex', input_dim=None):\n",
    "    \"\"\"\n",
    "    Main function to analyze model fairness\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing model: {os.path.basename(model_path)}\")\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model_or_dict, device, is_quantized, is_dict = load_model(model_path)\n",
    "        \n",
    "        # Get appropriate input dimension\n",
    "        if input_dim is None:\n",
    "            # Try to infer from model\n",
    "            if not is_dict:\n",
    "                try:\n",
    "                    input_dim = model_or_dict.fc1.weight.shape[1]\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                \n",
    "            # If quantized or couldn't get from model, try to infer from base model\n",
    "            if input_dim is None and is_quantized:\n",
    "                # Try to find base model\n",
    "                base_model_path = None\n",
    "                for suffix in ['_INT4.pth', '_INT8.pth', '_INT16.pth']:\n",
    "                    if suffix in model_path:\n",
    "                        potential_base = model_path.replace(suffix, '.pth')\n",
    "                        if os.path.exists(potential_base):\n",
    "                            base_model_path = potential_base\n",
    "                            break\n",
    "                \n",
    "                if base_model_path:\n",
    "                    try:\n",
    "                        base_state_dict = torch.load(base_model_path, map_location='cpu')\n",
    "                        input_dim = base_state_dict['fc1.weight'].shape[1]\n",
    "                    except Exception as e:\n",
    "                        input_dim = 14  # Default for Adult dataset\n",
    "                else:\n",
    "                    input_dim = 14  # Default for Adult dataset\n",
    "                    \n",
    "            if input_dim is None:\n",
    "                input_dim = 14  # Default for Adult dataset\n",
    "        \n",
    "        # Prepare dataset with correct features\n",
    "        dataset = prepare_adult_dataset(expected_features=input_dim, \n",
    "                                       protected_attribute=protected_attribute)\n",
    "        \n",
    "        # Compute fairness metrics\n",
    "        fairness_metrics = compute_fairness_metrics(\n",
    "            model_or_dict,\n",
    "            device,\n",
    "            dataset,\n",
    "            protected_attribute=protected_attribute,\n",
    "            privileged_groups=[{protected_attribute: 1}],\n",
    "            is_quantized=is_quantized,\n",
    "            is_dict=is_dict,\n",
    "            input_dim=input_dim\n",
    "        )\n",
    "        \n",
    "        return fairness_metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def format_model_name(path):\n",
    "    \"\"\"Format model name for display in tables\"\"\"\n",
    "    name = os.path.basename(path)\n",
    "    \n",
    "    # Simplify model names for better display\n",
    "    if 'fcnn_model_adult_income' in name:\n",
    "        if 'fcnn_model_adult_income_pruned_' in name:\n",
    "            bit = name.split('pruned_')[1].split('.')[0]\n",
    "            return f'B_PRU{bit}'\n",
    "        else:return 'Baseline'\n",
    "    elif 'adv_fcnn_model_adult_sex' in name:\n",
    "        if 'adv_fcnn_model_adult_sex_pruned_' in name:\n",
    "            bit = name.split('pruned_')[1].split('.')[0]\n",
    "            return f'A_PRU{bit}'\n",
    "        else:return 'Adversarial'\n",
    "    elif 'fair_demographic_parity_fcnn_model_adult_sex' in name:\n",
    "        if 'fair_demographic_parity_fcnn_model_adult_sex_pruned_' in name:\n",
    "            bit = name.split('pruned_')[1].split('.')[0]\n",
    "            return f'F_PRU{bit}'\n",
    "        else:return 'Fair_DP'\n",
    "    elif 'adult_baseline_INT' in name:\n",
    "        bit = name.split('INT')[1].split('.')[0]\n",
    "        return f'B_INT{bit}'\n",
    "    elif 'adult_adv_INT' in name:\n",
    "        bit = name.split('INT')[1].split('.')[0]\n",
    "        return f'A_INT{bit}'\n",
    "    elif 'adult_fair_dp_INT' in name:\n",
    "        bit = name.split('INT')[1].split('.')[0]\n",
    "        return f'F_INT{bit}'\n",
    "    elif 'fcnn_student_model_adult_income_' in name:\n",
    "        bit = name.split('income_')[1].split('.')[0]\n",
    "        return f'B_DIS{bit}'\n",
    "    elif 'adult_adv_DIS' in name:\n",
    "        bit = name.split('DIS')[1].split('.')[0]\n",
    "        return f'A_DIS{bit}'\n",
    "    elif 'adult_fair_dp_DIS' in name:\n",
    "        bit = name.split('DIS')[1].split('.')[0]\n",
    "        return f'F_DIS{bit}'\n",
    "    \n",
    "    return name\n",
    "\n",
    "def create_comparison_table(results, metrics_to_show=None):\n",
    "    \"\"\"\n",
    "    Create a side-by-side comparison table of fairness metrics across models\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to display\")\n",
    "        return\n",
    "    \n",
    "    # Default metrics to show\n",
    "    if metrics_to_show is None:\n",
    "        metrics_to_show = [\n",
    "            'disparate_impact',\n",
    "            'statistical_parity_difference',\n",
    "            'demographic_parity_difference',\n",
    "            'equalized_odds_difference',\n",
    "            'average_odds_difference',\n",
    "            'equal_opportunity_difference',\n",
    "            'theil_index'\n",
    "        ]\n",
    "    \n",
    "    # Initialize table headers\n",
    "    headers = ['Metric']\n",
    "    \n",
    "    # Add column for each model\n",
    "    models = []\n",
    "    for model_name in results.keys():\n",
    "        # Get the first attribute (usually 'sex')\n",
    "        for attr in results[model_name]:\n",
    "            if results[model_name][attr]:  # If metrics exist\n",
    "                models.append(model_name)\n",
    "                headers.append(format_model_name(model_name))\n",
    "                break\n",
    "    \n",
    "    # Create rows for each metric\n",
    "    table_data = []\n",
    "    for metric in metrics_to_show:\n",
    "        row = [metric.replace('_', ' ').title()]\n",
    "        \n",
    "        for model_name in models:\n",
    "            # Get first attribute (usually 'sex')\n",
    "            attr = next(iter(results[model_name].keys()))\n",
    "            \n",
    "            if results[model_name][attr] and metric in results[model_name][attr]:\n",
    "                value = results[model_name][attr][metric]\n",
    "                # Format the value\n",
    "                row.append(f\"{value:.4f}\")\n",
    "            else:\n",
    "                row.append(\"N/A\")\n",
    "        \n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Print the table\n",
    "    print(\"\\n===== FAIRNESS METRICS COMPARISON =====\")\n",
    "    \n",
    "    # Use tabulate if available, otherwise use simple formatting\n",
    "    if HAS_TABULATE:\n",
    "        print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "    else:\n",
    "        # Simple table formatting\n",
    "        header_str = \"| \" + \" | \".join(headers) + \" |\"\n",
    "        divider = \"-\" * len(header_str)\n",
    "        print(divider)\n",
    "        print(header_str)\n",
    "        print(divider)\n",
    "        \n",
    "        for row in table_data:\n",
    "            row_str = \"| \" + \" | \".join(str(cell) for cell in row) + \" |\"\n",
    "            print(row_str)\n",
    "            \n",
    "        print(divider)\n",
    "    \n",
    "    # Return a dataframe for further analysis if needed\n",
    "    return pd.DataFrame(table_data, columns=headers).set_index(headers[0])\n",
    "\n",
    "def run_comprehensive_analysis(model_paths, protected_attributes=['sex']):\n",
    "    \"\"\"\n",
    "    Create comparative analysis across all models and attributes\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # First analyze standard models to get input dimensions\n",
    "    # This helps with quantized models later\n",
    "    input_dims = {}\n",
    "    \n",
    "    # First pass to collect dimensions from standard models\n",
    "    for model_path in model_paths:\n",
    "        if os.path.exists(model_path):  # Skip non-existent models\n",
    "            model_name = os.path.basename(model_path)\n",
    "            if not any(q in model_path for q in ['INT4', 'INT8', 'INT16']):\n",
    "                try:\n",
    "                    state_dict = torch.load(model_path, map_location='cpu')\n",
    "                    base_name = model_name.split('.')[0]  # Remove extension\n",
    "                    input_dims[base_name] = state_dict['fc1.weight'].shape[1]\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "    \n",
    "    # Now run full analysis with dimensions information\n",
    "    for model_path in model_paths:\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Skipping non-existent model: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        model_name = os.path.basename(model_path)\n",
    "        results[model_name] = {}\n",
    "        \n",
    "        # For quantized models, try to find the input dimension\n",
    "        input_dim = None\n",
    "        if any(q in model_path for q in ['INT4', 'INT8', 'INT16']):\n",
    "            # Extract base model name by removing the INT part\n",
    "            for suffix in ['_INT4', '_INT8', '_INT16']:\n",
    "                if suffix in model_name:\n",
    "                    base_name = model_name.split(suffix)[0]\n",
    "                    if base_name in input_dims:\n",
    "                        input_dim = input_dims[base_name]\n",
    "                        break\n",
    "        \n",
    "        for attr in protected_attributes:\n",
    "            try:\n",
    "                fairness_metrics = analyze_model_fairness(model_path, protected_attribute=attr, input_dim=input_dim)\n",
    "                results[model_name][attr] = fairness_metrics\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {model_name} with {attr}: {str(e)}\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = create_comparison_table(results)\n",
    "    \n",
    "    return results, comparison_df\n",
    "\n",
    "# Function to generate clean path that exists in the system\n",
    "def get_valid_path(base_paths, filename):\n",
    "    \"\"\"Try multiple base paths to find one that exists with the given filename\"\"\"\n",
    "    potential_paths = [\n",
    "        os.path.join(path, filename) for path in base_paths\n",
    "    ]\n",
    "    \n",
    "    for path in potential_paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    \n",
    "    # If no path exists, return the first one (which will fail later with a clear error)\n",
    "    return potential_paths[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07fb6775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of all models...\n",
      "\n",
      "Analyzing model: fcnn_model_adult_income.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:536: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_baseline_INT4.pth\n",
      "WARNING: Could not load state dict, using random weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_baseline_INT8.pth\n",
      "WARNING: Could not load state dict, using random weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_baseline_INT16.pth\n",
      "WARNING: Could not load state dict, using random weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: fcnn_student_model_adult_income_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: fcnn_student_model_adult_income_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: fcnn_model_adult_income_pruned_20pct.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: fcnn_model_adult_income_pruned_40pct.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: fcnn_model_adult_income_pruned_60pct.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: fair_demographic_parity_fcnn_model_adult_sex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_fair_dp_INT4.pth\n",
      "WARNING: Could not load state dict, using random weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_fair_dp_INT8.pth\n",
      "WARNING: Could not load state dict, using random weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_fair_dp_INT16.pth\n",
      "WARNING: Could not load state dict, using random weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_fair_dp_DIS4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_fair_dp_DIS8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: fair_demographic_parity_fcnn_model_adult_sex_pruned_20pct.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: fair_demographic_parity_fcnn_model_adult_sex_pruned_40pct.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: fair_demographic_parity_fcnn_model_adult_sex_pruned_60pct.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adv_fcnn_model_adult_sex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_adv_INT4.pth\n",
      "WARNING: Could not load state dict, using random weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_adv_INT8.pth\n",
      "WARNING: Could not load state dict, using random weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_adv_INT16.pth\n",
      "WARNING: Could not load state dict, using random weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_adv_DIS4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adult_adv_DIS8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adv_fcnn_model_adult_sex_pruned_20pct.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adv_fcnn_model_adult_sex_pruned_40pct.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model: adv_fcnn_model_adult_sex_pruned_60pct.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Temp\\ipykernel_41288\\328194755.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FAIRNESS METRICS COMPARISON =====\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Metric                        |   Baseline |   B_INT4 |   B_INT8 |   B_INT16 |   B_DIS4 |   B_DIS8 |   B_PRU20pct |   B_PRU40pct |   B_PRU60pct |   Fair_DP |   F_INT4 |   F_INT8 |   F_INT16 |   F_DIS4 |   F_DIS8 |   F_PRU20pct |   F_PRU40pct |   F_PRU60pct |   Adversarial |   A_INT4 |   A_INT8 |   A_INT16 |   A_DIS4 |   A_DIS8 |   A_PRU20pct |   A_PRU40pct |   A_PRU60pct |\n",
      "+===============================+============+==========+==========+===========+==========+==========+==============+==============+==============+===========+==========+==========+===========+==========+==========+==============+==============+==============+===============+==========+==========+===========+==========+==========+==============+==============+==============+\n",
      "| Disparate Impact              |     0.4817 |   0.9996 |   0.9601 |    1.6953 |   0.4622 |   0.8661 |       0.3666 |       0.5504 |       1.2937 |    0.3861 |   0.7514 |   0.8992 |    1.0764 |   0.4501 |   0.4295 |       0.4012 |       0.3435 |       0.2615 |        0.2084 |   1.0109 |   0.6676 |    0.9892 |   0.1714 |   0.5383 |       0.2075 |       0.1455 |       0.0181 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Statistical Parity Difference |    -0.0301 |  -0.0004 |  -0.0367 |    0.05   |  -0.0312 |  -0.1087 |      -0.055  |      -0.0654 |       0.0031 |   -0.032  |  -0.0232 |  -0.0424 |    0.0407 |  -0.0346 |  -0.0391 |      -0.0298 |      -0.0415 |      -0.0378 |       -0.2852 |   0.0108 |  -0.1634 |   -0.0103 |  -0.6425 |  -0.0168 |      -0.2142 |      -0.2193 |      -0.0504 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Demographic Parity Difference |     0.0301 |   0.0004 |   0.0367 |    0.05   |   0.0312 |   0.1087 |       0.055  |       0.0654 |       0.0031 |    0.032  |   0.0232 |   0.0424 |    0.0407 |   0.0346 |   0.0391 |       0.0298 |       0.0415 |       0.0378 |        0.2852 |   0.0108 |   0.1634 |    0.0103 |   0.6425 |   0.0168 |       0.2142 |       0.2193 |       0.0504 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Equalized Odds Difference     |     0.0337 |   0.0004 |   0.0483 |    0.0414 |   0.0381 |   0.1122 |       0.0225 |       0.0422 |       0.087  |    0.0005 |   0.0092 |   0.0465 |    0.0468 |   0.0368 |   0.0227 |       0.0065 |       0.0134 |       0.0425 |        0.262  |   0.0127 |   0.1765 |    0.0101 |   0.6615 |   0.0536 |       0.2182 |       0.1914 |       0.0612 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Average Odds Difference       |     0.0162 |  -0.0002 |  -0.0439 |    0.0366 |   0.0157 |  -0.0873 |      -0.0114 |      -0.0247 |       0.0435 |   -0.0004 |   0.002  |  -0.0065 |    0.023  |   0.0124 |   0.0066 |       0.0033 |      -0.0039 |      -0.023  |       -0.2323 |   0.0097 |  -0.1731 |   -0.0046 |  -0.5692 |   0.0267 |      -0.156  |      -0.1859 |      -0.0427 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Equal Opportunity Difference  |     0.0337 |   0      |  -0.0395 |    0.0319 |   0.0381 |  -0.0623 |      -0.0002 |      -0.0073 |       0.087  |   -0.0005 |   0.0092 |  -0.0465 |    0.0468 |   0.0368 |   0.0227 |       0.0065 |       0.0055 |      -0.0425 |       -0.2027 |   0.0067 |  -0.1765 |   -0.0101 |  -0.477  |   0.0536 |      -0.0938 |      -0.1805 |      -0.0241 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Theil Index                   |     0.2229 |   0.0334 |   0.0686 |    0.2932 |   0.2271 |   0.0963 |       0.219  |       0.228  |       0.2611 |    0.2257 |   0.2443 |   0.1436 |    0.2083 |   0.2291 |   0.2218 |       0.2271 |       0.2267 |       0.23   |        0.1931 |   0.0359 |   0.2045 |    0.0403 |   0.1322 |   0.2374 |       0.2297 |       0.2113 |       0.2814 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "\n",
      "===== MODEL COMPARISON BY GROUP =====\n",
      "\n",
      "BASELINE GROUP COMPARISON:\n",
      "\n",
      "===== FAIRNESS METRICS COMPARISON =====\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Metric                        |   Baseline |   B_INT4 |   B_INT8 |   B_INT16 |   B_DIS4 |   B_DIS8 |   B_PRU20pct |   B_PRU40pct |   B_PRU60pct |\n",
      "+===============================+============+==========+==========+===========+==========+==========+==============+==============+==============+\n",
      "| Disparate Impact              |     0.4817 |   0.9996 |   0.9601 |    1.6953 |   0.4622 |   0.8661 |       0.3666 |       0.5504 |       1.2937 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Statistical Parity Difference |    -0.0301 |  -0.0004 |  -0.0367 |    0.05   |  -0.0312 |  -0.1087 |      -0.055  |      -0.0654 |       0.0031 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Demographic Parity Difference |     0.0301 |   0.0004 |   0.0367 |    0.05   |   0.0312 |   0.1087 |       0.055  |       0.0654 |       0.0031 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Equalized Odds Difference     |     0.0337 |   0.0004 |   0.0483 |    0.0414 |   0.0381 |   0.1122 |       0.0225 |       0.0422 |       0.087  |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Average Odds Difference       |     0.0162 |  -0.0002 |  -0.0439 |    0.0366 |   0.0157 |  -0.0873 |      -0.0114 |      -0.0247 |       0.0435 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Equal Opportunity Difference  |     0.0337 |   0      |  -0.0395 |    0.0319 |   0.0381 |  -0.0623 |      -0.0002 |      -0.0073 |       0.087  |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Theil Index                   |     0.2229 |   0.0334 |   0.0686 |    0.2932 |   0.2271 |   0.0963 |       0.219  |       0.228  |       0.2611 |\n",
      "+-------------------------------+------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "\n",
      "FAIR_DP GROUP COMPARISON:\n",
      "\n",
      "===== FAIRNESS METRICS COMPARISON =====\n",
      "+-------------------------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Metric                        |   Fair_DP |   F_INT4 |   F_INT8 |   F_INT16 |   F_DIS4 |   F_DIS8 |   F_PRU20pct |   F_PRU40pct |   F_PRU60pct |\n",
      "+===============================+===========+==========+==========+===========+==========+==========+==============+==============+==============+\n",
      "| Disparate Impact              |    0.3861 |   0.7514 |   0.8992 |    1.0764 |   0.4501 |   0.4295 |       0.4012 |       0.3435 |       0.2615 |\n",
      "+-------------------------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Statistical Parity Difference |   -0.032  |  -0.0232 |  -0.0424 |    0.0407 |  -0.0346 |  -0.0391 |      -0.0298 |      -0.0415 |      -0.0378 |\n",
      "+-------------------------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Demographic Parity Difference |    0.032  |   0.0232 |   0.0424 |    0.0407 |   0.0346 |   0.0391 |       0.0298 |       0.0415 |       0.0378 |\n",
      "+-------------------------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Equalized Odds Difference     |    0.0005 |   0.0092 |   0.0465 |    0.0468 |   0.0368 |   0.0227 |       0.0065 |       0.0134 |       0.0425 |\n",
      "+-------------------------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Average Odds Difference       |   -0.0004 |   0.002  |  -0.0065 |    0.023  |   0.0124 |   0.0066 |       0.0033 |      -0.0039 |      -0.023  |\n",
      "+-------------------------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Equal Opportunity Difference  |   -0.0005 |   0.0092 |  -0.0465 |    0.0468 |   0.0368 |   0.0227 |       0.0065 |       0.0055 |      -0.0425 |\n",
      "+-------------------------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Theil Index                   |    0.2257 |   0.2443 |   0.1436 |    0.2083 |   0.2291 |   0.2218 |       0.2271 |       0.2267 |       0.23   |\n",
      "+-------------------------------+-----------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "\n",
      "ADVERSARIAL GROUP COMPARISON:\n",
      "\n",
      "===== FAIRNESS METRICS COMPARISON =====\n",
      "+-------------------------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Metric                        |   Adversarial |   A_INT4 |   A_INT8 |   A_INT16 |   A_DIS4 |   A_DIS8 |   A_PRU20pct |   A_PRU40pct |   A_PRU60pct |\n",
      "+===============================+===============+==========+==========+===========+==========+==========+==============+==============+==============+\n",
      "| Disparate Impact              |        0.2084 |   1.0109 |   0.6676 |    0.9892 |   0.1714 |   0.5383 |       0.2075 |       0.1455 |       0.0181 |\n",
      "+-------------------------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Statistical Parity Difference |       -0.2852 |   0.0108 |  -0.1634 |   -0.0103 |  -0.6425 |  -0.0168 |      -0.2142 |      -0.2193 |      -0.0504 |\n",
      "+-------------------------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Demographic Parity Difference |        0.2852 |   0.0108 |   0.1634 |    0.0103 |   0.6425 |   0.0168 |       0.2142 |       0.2193 |       0.0504 |\n",
      "+-------------------------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Equalized Odds Difference     |        0.262  |   0.0127 |   0.1765 |    0.0101 |   0.6615 |   0.0536 |       0.2182 |       0.1914 |       0.0612 |\n",
      "+-------------------------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Average Odds Difference       |       -0.2323 |   0.0097 |  -0.1731 |   -0.0046 |  -0.5692 |   0.0267 |      -0.156  |      -0.1859 |      -0.0427 |\n",
      "+-------------------------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Equal Opportunity Difference  |       -0.2027 |   0.0067 |  -0.1765 |   -0.0101 |  -0.477  |   0.0536 |      -0.0938 |      -0.1805 |      -0.0241 |\n",
      "+-------------------------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n",
      "| Theil Index                   |        0.1931 |   0.0359 |   0.2045 |    0.0403 |   0.1322 |   0.2374 |       0.2297 |       0.2113 |       0.2814 |\n",
      "+-------------------------------+---------------+----------+----------+-----------+----------+----------+--------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Base paths to search\n",
    "base_paths = ['../models', '../../models']\n",
    "\n",
    "# Group models by model type\n",
    "grouped_models = {\n",
    "    'baseline': [\n",
    "        'baseline/fcnn_model_adult_income.pth',\n",
    "        'quantized/adult_baseline_INT4.pth',\n",
    "        'quantized/adult_baseline_INT8.pth',\n",
    "        'quantized/adult_baseline_INT16.pth',\n",
    "        'distillation/fcnn_student_model_adult_income_4.pth',\n",
    "        'distillation/fcnn_student_model_adult_income_8.pth',\n",
    "        'pruned/fcnn_model_adult_income_pruned_20pct.pth',\n",
    "        'pruned/fcnn_model_adult_income_pruned_40pct.pth',\n",
    "        'pruned/fcnn_model_adult_income_pruned_60pct.pth'\n",
    "    ],\n",
    "    'fair_dp': [\n",
    "        'debiased/fair_demographic_parity_fcnn_model_adult_sex.pth',\n",
    "        'quantized/adult_fair_dp_INT4.pth',\n",
    "        'quantized/adult_fair_dp_INT8.pth',\n",
    "        'quantized/adult_fair_dp_INT16.pth',\n",
    "        'distillation/adult_fair_dp_DIS4.pth',\n",
    "        'distillation/adult_fair_dp_DIS8.pth',\n",
    "        'pruned/fair_demographic_parity_fcnn_model_adult_sex_pruned_20pct.pth',\n",
    "        'pruned/fair_demographic_parity_fcnn_model_adult_sex_pruned_40pct.pth',\n",
    "        'pruned/fair_demographic_parity_fcnn_model_adult_sex_pruned_60pct.pth'\n",
    "    ],\n",
    "    'adversarial': [\n",
    "        'debiased/adv_fcnn_model_adult_sex.pth',\n",
    "        'quantized/adult_adv_INT4.pth',\n",
    "        'quantized/adult_adv_INT8.pth',\n",
    "        'quantized/adult_adv_INT16.pth',\n",
    "        'distillation/adult_adv_DIS4.pth',\n",
    "        'distillation/adult_adv_DIS8.pth',\n",
    "        'pruned/adv_fcnn_model_adult_sex_pruned_20pct.pth',\n",
    "        'pruned/adv_fcnn_model_adult_sex_pruned_40pct.pth',\n",
    "        'pruned/adv_fcnn_model_adult_sex_pruned_60pct.pth'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create paths with valid directories\n",
    "model_paths = []\n",
    "for group, filenames in grouped_models.items():\n",
    "    for filename in filenames:\n",
    "        model_paths.append(get_valid_path(base_paths, filename))\n",
    "\n",
    "print(\"Starting analysis of all models...\")\n",
    "# Run analysis for a single protected attribute\n",
    "results, comparison_df = run_comprehensive_analysis(model_paths, protected_attributes=['sex'])\n",
    "\n",
    "# Display additional group-based comparisons\n",
    "print(\"\\n===== MODEL COMPARISON BY GROUP =====\")\n",
    "for group_name, group_files in grouped_models.items():\n",
    "    group_results = {}\n",
    "    for path in model_paths:\n",
    "        basename = os.path.basename(path)\n",
    "        for file in group_files:\n",
    "            if file.split('/')[-1] == basename:\n",
    "                if basename in results:\n",
    "                    group_results[basename] = results[basename]\n",
    "    \n",
    "    if group_results:\n",
    "        print(f\"\\n{group_name.upper()} GROUP COMPARISON:\")\n",
    "        create_comparison_table(group_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
